---
title: "Introduction to roclab"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to roclab}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(roclab)
```

# Overview
The **roclab** package provides an implementation of direct AUC maximization
for binary classification, supporting both linear and kernel-based models.

- Linear models allow different regularization penalties (ridge, lasso, elastic net, etc.).

- Kernel models allow flexible kernel functions (radial, polynomial, linear, laplace).

- Both models support multiple loss functions (hinge, hinge2 (squared hinge), logistic, exponential).

Efficient optimization is implemented (e.g., **Adamax gradient descent**).  
For large datasets, scalability is achieved by  
using only sampled pairs of positive–negative pairs (**incomplete U-statistics**)  
and, for kernel models, further applying a **Nyström low-rank approximation** to the kernel matrix.

# Example: Linear Model
```{r}
set.seed(123)
n_lin <- 1500
n_pos_lin <- round(0.2 * n_lin)
n_neg_lin <- n_lin - n_pos_lin

X_train_lin <- rbind(
  matrix(rnorm(2 * n_neg_lin, mean = -1), ncol = 2),
  matrix(rnorm(2 * n_pos_lin, mean =  1), ncol = 2)
)
y_train_lin <- c(rep(-1, n_neg_lin), rep(1, n_pos_lin))

# Fit linear model
fit_lin <- roclearn(X_train_lin, y_train_lin, lambda = 0.1)

# Summary
summary(fit_lin)

# Test set
n_test_lin <- 300
n_pos_test_lin <- round(0.2 * n_test_lin)
n_neg_test_lin <- n_test_lin - n_pos_test_lin
X_test_lin <- rbind(
  matrix(rnorm(2 * n_neg_test_lin, mean = -1), ncol = 2),
  matrix(rnorm(2 * n_pos_test_lin, mean =  1), ncol = 2)
)
y_test_lin <- c(rep(-1, n_neg_test_lin), rep(1, n_pos_test_lin))

# Predict
pred_class_lin <- predict(fit_lin, X_test_lin, type = "response")
pred_score_lin <- predict(fit_lin, X_test_lin, type = "class")

# AUC
auc(fit_lin, X_test_lin, y_test_lin)
```

```{r, fig.width=7, fig.height=6}
# Plot decision boundary
plot(fit_lin, newdata = X_test_lin, y = y_test_lin, features = c(1, 2))
```

# Example: Kernel Model
```{r}
set.seed(123)
n_ker <- 1500
r_train_ker <- sqrt(runif(n_ker, 0.05, 1))
theta_train_ker <- runif(n_ker, 0, 2*pi)
X_train_ker <- cbind(r_train_ker * cos(theta_train_ker), r_train_ker * sin(theta_train_ker))
y_train_ker <- ifelse(r_train_ker < 0.5, 1, -1)

# Fit kernel model
fit_ker <- kroclearn(X_train_ker, y_train_ker, lambda = 0.1, kernel = "radial")

# Summary
summary(fit_ker)

# Test set
n_test_ker <- 300
r_test_ker <- sqrt(runif(n_test_ker, 0.05, 1))
theta_test_ker <- runif(n_test_ker, 0, 2*pi)
X_test_ker <- cbind(r_test_ker * cos(theta_test_ker), r_test_ker * sin(theta_test_ker))
y_test_ker <- ifelse(r_test_ker < 0.5, 1, -1)

# Predict
scores_ker <- predict(fit_ker, X_test_ker, type = "response")
classes_ker <- predict(fit_ker, X_test_ker, type = "class")

# AUC
auc(fit_ker, X_test_ker, y_test_ker)
```

```{r, fig.width=7, fig.height=6}
# Plot decision boundary
plot(fit_ker, newdata = X_test_ker, y = y_test_ker, features = c(1, 2))
```

# Cross-Validation
```{r}
# Linear CV
cvfit_lin <- cv.roclearn(
  X_train_lin, y_train_lin,
  lambda.vec = exp(seq(log(0.01), log(5), length.out = 5)),
  nfolds = 5
)

# Summarize cross-validation results
summary(cvfit_lin)
```

```{r, fig.width=7, fig.height=6}
# Plot the cross-validation AUC curve across lambda values
plot(cvfit_lin)
```
```{r}
# Kernel CV
cvfit_ker <- cv.kroclearn(
  X_train_ker, y_train_ker,
  lambda.vec = exp(seq(log(0.01), log(5), length.out = 5)),
  kernel = "radial",
  nfolds = 5
)

# Summarize cross-validation results
summary(cvfit_ker)
```

```{r, fig.width=7, fig.height=6}
# Plot the cross-validation AUC curve across lambda values
plot(cvfit_ker)
```

# Conclusion

The **roclab** package provides tools for direct AUC maximization in binary classification,
with **linear models** (supporting regularization penalties) and **kernel models** (supporting kernel functions).
Both models allow multiple loss functions and scalable training for large, imbalanced datasets.
