% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv.roclearn.R
\name{cv.roclearn}
\alias{cv.roclearn}
\title{Cross-validation for Linear AUC Maximization Models}
\usage{
cv.roclearn(
  X,
  y,
  lambda.vec = NULL,
  lambda.length = 50,
  penalty = "ridge",
  param.penalty = NULL,
  loss = "hinge",
  approx = NULL,
  intercept = TRUE,
  nfolds = 10,
  target.perf = list(),
  param.convergence = list()
)
}
\arguments{
\item{X}{Predictor matrix or data.frame (categorical variables are
automatically one-hot encoded).}

\item{y}{Response vector with class labels in \{-1, 1\}. Labels given as
\{0,1\} or as a two-level factor/character are coerced automatically.}

\item{lambda.vec}{Optional numeric vector of penalty values. If \code{NULL},
a decreasing sequence is generated automatically.}

\item{lambda.length}{Number of \eqn{\lambda} values to generate if
\code{lambda.vec} is \code{NULL}.}

\item{penalty}{Regularization penalty type: \code{"ridge"}, \code{"lasso"},
\code{"elastic"}, \code{"alasso"}, \code{"scad"}, or \code{"mcp"}.}

\item{param.penalty}{Penalty-specific parameter (ignored for ridge/lasso/alasso).}

\item{loss}{Loss function: \code{"hinge"}, \code{"hinge2"} (squared hinge),
\code{"logistic"}, or \code{"exponential"}.}

\item{approx}{Logical; if \code{TRUE}, train the linear model using
a subsampled set of positive–negative pairs (incomplete U-statistic).
This substantially reduces computational cost for large datasets.
Default is \code{TRUE} when \code{nrow(X) >= 1000}.}

\item{intercept}{Logical; include an intercept in the model (default \code{TRUE}).}

\item{nfolds}{Number of cross-validation folds (default 10).}

\item{target.perf}{List of target performance criteria (passed to
\code{roclearn}).}

\item{param.convergence}{List of convergence controls (e.g., \code{maxiter},
\code{eps}).}
}
\value{
An object of class \code{"cv.roclearn"} with:
\itemize{
\item \code{optimal.lambda} — selected \eqn{\lambda}.
\item \code{optimal.fit} — model refit on the full data at
\code{optimal.lambda}.
\item \code{lambda.vec} — grid of penalty values considered.
\item \code{auc.mean}, \code{auc.sd} — mean and sd of cross-validated AUC.
\item \code{auc.result} — fold-by-lambda AUC matrix.
\item \code{time.mean}, \code{time.sd} — mean and sd of training time.
\item \code{time.result} — fold-by-lambda training time matrix.
\item \code{nfolds}, \code{loss}, \code{penalty} — settings.
}
}
\description{
Perform k-fold cross-validation over a sequence of \eqn{\lambda} values
and select the optimal linear model based on cross-validated AUC.
}
\examples{
\dontrun{
## Linear model example
set.seed(123)
n <- 1500
n_pos <- round(0.2 * n)
n_neg <- n - n_pos

X <- rbind(
  matrix(rnorm(2 * n_neg, mean = -1), ncol = 2),
  matrix(rnorm(2 * n_pos, mean =  1), ncol = 2)
)
y <- c(rep(-1, n_neg), rep(1, n_pos))

cvfit <- cv.roclearn(
  X, y,
  lambda.vec = exp(seq(log(0.01), log(5), length.out = 5)),
  nfolds = 5
)
cvfit$optimal.lambda
}
}
