% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kroclearn.R
\name{kroclearn}
\alias{kroclearn}
\title{Fit a Kernel AUC Maximization Model}
\usage{
kroclearn(
  X,
  y,
  lambda,
  kernel = "radial",
  param.kernel = NULL,
  loss = "hinge",
  approx = NULL,
  intercept = TRUE,
  target.perf = list(),
  param.convergence = list()
)
}
\arguments{
\item{X}{Predictor matrix or data.frame. Categorical variables are automatically
one-hot encoded (first level dropped).}

\item{y}{Class labels in \{-1, 1\}. Labels given as \{0,1\} or as two-level
factors/characters are coerced automatically.}

\item{lambda}{Positive scalar regularization parameter.}

\item{kernel}{Kernel type: \code{"radial"} (default), \code{"polynomial"},
\code{"linear"}, or \code{"laplace"}.}

\item{param.kernel}{Kernel-specific parameter:
\itemize{
\item \eqn{\sigma} for \code{"radial"} and \code{"laplace"} kernels
(default \eqn{1/p}, where \eqn{p} is the number of predictors after preprocessing,
i.e., after categorical variables are one-hot encoded).
\item Degree for \code{"polynomial"} kernel (default 2).
\item Ignored for \code{"linear"} kernel.
}}

\item{loss}{Loss function: \code{"hinge"} (default), \code{"hinge2"} (squared hinge),
\code{"logistic"}, or \code{"exponential"}.}

\item{approx}{Logical; if \code{TRUE}, train the kernel model using
subsampled positive–negative pairs (incomplete U-statistic), and further
apply a Nyström approximation to the kernel matrix. This reduces memory and
time cost for large datasets.
Default is \code{TRUE} when \code{nrow(X) >= 1000}, otherwise \code{FALSE}.}

\item{intercept}{Logical; if \code{TRUE}, estimate an intercept term
(default \code{TRUE}).}

\item{target.perf}{List with target sensitivity and specificity used when
estimating the intercept (defaults to 0.9 each).}

\item{param.convergence}{List of convergence controls (e.g., \code{maxiter},
\code{eps}). Default is \code{list(maxiter = 5e4, eps = 1e-4)}.}
}
\value{
An object of class \code{"kroclearn"}, a list containing:
\itemize{
\item \code{theta.hat} — estimated dual coefficient vector.
\item \code{intercept} — fitted intercept (if applicable).
\item \code{lambda}, \code{kernel}, \code{param.kernel}, \code{loss}.
\item \code{approx}, \code{B} (number of sampled pairs if approximation used).
\item \code{time} — training time (seconds).
\item \code{nobs}, \code{p} — number of observations and predictors.
\item \code{converged}, \code{n.iter} — convergence information.
\item \code{kfunc} — kernel function object.
\item \code{nystrom} — Nyström approximation details (if used).
\item \code{X} — training data (post-preprocessing).
\item \code{preprocessing} — details on categorical variables,
removed columns, and column names.
\item \code{call} — the function call.
}
}
\description{
Train a kernel-based model that directly maximizes the Area Under the ROC Curve (AUC).
Several kernel types and loss functions are supported.
For large datasets, the algorithm can speed up training by using only a sampled subset of
positive–negative pairs (incomplete U-statistic approximation), together with a low-rank
approximation of the kernel matrix (Nyström approximation).
}
\examples{
\donttest{
set.seed(123)
n <- 1500
r <- sqrt(runif(n, 0.05, 1))
theta <- runif(n, 0, 2*pi)
X <- cbind(r * cos(theta), r * sin(theta))
y <- ifelse(r < 0.5, 1, -1)

fit <- kroclearn(X, y, lambda = 0.1, kernel = "radial")
}
}
